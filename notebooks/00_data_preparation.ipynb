{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preparation for Retail/CPG Planning Analytics\n",
        "\n",
        "This notebook generates and prepares realistic datasets for a **global retail/CPG company's planning value chain**. The datasets cover end-to-end planning processes from demand planning to supply planning.\n",
        "\n",
        "**Planning Value Chain Covered:**\n",
        "- Demand Planning\n",
        "- Supply Planning\n",
        "- Inventory Netting\n",
        "- Production Planning\n",
        "- Material Planning\n",
        "- Distribution Requirements Planning (DRP)\n",
        "\n",
        "**Datasets Generated:**\n",
        "\n",
        "| Dataset | Type | Use Case | Notebook |\n",
        "|---------|------|----------|----------|\n",
        "| `supplier_delay_risk` | Classification | Predict supplier delivery delays | 01_classification |\n",
        "| `material_shortage` | Multi-class Classification | Predict material shortage risk levels | 01_classification |\n",
        "| `price_elasticity` | Regression | Predict price elasticity of demand | 02_regression |\n",
        "| `promotion_lift` | Regression | Predict promotional sales lift | 02_regression |\n",
        "| `scrap_anomaly` | Anomaly Detection | Detect unusual scrap/defect patterns | 03_outlier_detection |\n",
        "| `demand_forecast` | Time Series | Forecast product demand by region | 04_time_series_forecasting |\n",
        "\n",
        "**Run this notebook once** to set up all the data before running the analytics notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute Setup\n",
        "\n",
        "We recommend running this notebook on **Serverless Compute** with the **Base Environment V4**.\n",
        "\n",
        "To configure:\n",
        "1. Click on the compute selector in the notebook toolbar\n",
        "2. Select **Serverless**\n",
        "3. Under Environment, choose **Base Environment V4**\n",
        "\n",
        "Serverless compute provides fast startup times and automatic scaling, ideal for interactive notebook workflows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install pandas numpy --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dbutils.library.restartPython()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "\n",
        "Define the catalog and schema where datasets will be stored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure your catalog and schema\n",
        "CATALOG = \"tabpfn_databricks\"\n",
        "SCHEMA = \"default\"\n",
        "\n",
        "# Create the catalog and schema if they don't exist\n",
        "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
        "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
        "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
        "spark.sql(f\"USE SCHEMA {SCHEMA}\")\n",
        "\n",
        "print(f\"Using catalog: {CATALOG}\")\n",
        "print(f\"Using schema: {SCHEMA}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Import Data Generation Utilities\n",
        "\n",
        "We use custom data generation functions that create realistic retail/CPG planning datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the scripts directory to the path so we can import util.py\n",
        "# In Databricks, you may need to adjust this path based on your repo structure\n",
        "repo_root = os.path.dirname(os.getcwd()) if 'notebooks' in os.getcwd() else os.getcwd()\n",
        "scripts_path = os.path.join(repo_root, 'scripts')\n",
        "if scripts_path not in sys.path:\n",
        "    sys.path.insert(0, scripts_path)\n",
        "\n",
        "# Import data generation functions\n",
        "from util import (\n",
        "    generate_supplier_delay_risk_data,\n",
        "    generate_material_shortage_data,\n",
        "    generate_price_elasticity_data,\n",
        "    generate_promotion_lift_data,\n",
        "    generate_scrap_anomaly_data,\n",
        "    generate_aggregate_demand_forecast_data\n",
        ")\n",
        "\n",
        "print(\"Data generation utilities imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Supplier Delay Risk Dataset (Binary Classification)\n",
        "\n",
        "**Use Case:** Supply Planning - Predict which supplier deliveries are at risk of delay\n",
        "\n",
        "This dataset helps supply planners identify high-risk orders and take proactive mitigation actions such as expediting, finding alternative suppliers, or adjusting production schedules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate supplier delay risk data\n",
        "df_supplier_delay = generate_supplier_delay_risk_data(n_samples=2000, seed=42)\n",
        "\n",
        "print(f\"Supplier Delay Risk Dataset\")\n",
        "print(f\"Shape: {df_supplier_delay.shape}\")\n",
        "print(f\"\\nTarget distribution (is_delayed):\")\n",
        "print(df_supplier_delay['is_delayed'].value_counts())\n",
        "print(f\"\\nDelay rate: {df_supplier_delay['is_delayed'].mean():.1%}\")\n",
        "\n",
        "display(df_supplier_delay.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save to Delta table\n",
        "spark.createDataFrame(df_supplier_delay).write.mode(\"overwrite\").saveAsTable(\"supplier_delay_risk\")\n",
        "print(f\"✓ Saved to {CATALOG}.{SCHEMA}.supplier_delay_risk\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Material Shortage Dataset (Multi-class Classification)\n",
        "\n",
        "**Use Case:** Material Planning - Predict which materials are at risk of shortage\n",
        "\n",
        "This dataset supports material planners in prioritizing procurement actions based on shortage risk levels (No Risk, At Risk, Critical)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate material shortage data\n",
        "df_material_shortage = generate_material_shortage_data(n_samples=1500, seed=42)\n",
        "\n",
        "print(f\"Material Shortage Dataset\")\n",
        "print(f\"Shape: {df_material_shortage.shape}\")\n",
        "print(f\"\\nTarget distribution (shortage_risk):\")\n",
        "print(\"0 = No Risk, 1 = At Risk, 2 = Critical\")\n",
        "print(df_material_shortage['shortage_risk'].value_counts().sort_index())\n",
        "\n",
        "display(df_material_shortage.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save to Delta table\n",
        "spark.createDataFrame(df_material_shortage).write.mode(\"overwrite\").saveAsTable(\"material_shortage\")\n",
        "print(f\"✓ Saved to {CATALOG}.{SCHEMA}.material_shortage\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Price Elasticity Dataset (Regression)\n",
        "\n",
        "**Use Case:** Demand Planning - Understand how price changes affect demand\n",
        "\n",
        "This dataset helps demand planners and revenue management teams predict how changes in pricing will impact unit sales across different products and market conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate price elasticity data\n",
        "df_price_elasticity = generate_price_elasticity_data(n_samples=3000, seed=42)\n",
        "\n",
        "print(f\"Price Elasticity Dataset\")\n",
        "print(f\"Shape: {df_price_elasticity.shape}\")\n",
        "print(f\"\\nTarget (price_elasticity) statistics:\")\n",
        "print(df_price_elasticity['price_elasticity'].describe())\n",
        "print(f\"\\nNote: Elasticity values are typically negative (demand decreases as price increases)\")\n",
        "print(f\"More negative = more elastic (price sensitive)\")\n",
        "\n",
        "display(df_price_elasticity.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save to Delta table\n",
        "spark.createDataFrame(df_price_elasticity).write.mode(\"overwrite\").saveAsTable(\"price_elasticity\")\n",
        "print(f\"✓ Saved to {CATALOG}.{SCHEMA}.price_elasticity\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Promotion Lift Dataset (Regression)\n",
        "\n",
        "**Use Case:** Demand Planning - Predict the sales impact of planned promotions\n",
        "\n",
        "This dataset supports trade promotion planning by predicting the incremental sales lift from different promotion types, depths, and marketing support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate promotion lift data\n",
        "df_promotion_lift = generate_promotion_lift_data(n_samples=2500, seed=42)\n",
        "\n",
        "print(f\"Promotion Lift Dataset\")\n",
        "print(f\"Shape: {df_promotion_lift.shape}\")\n",
        "print(f\"\\nTarget (promotion_lift_pct) statistics:\")\n",
        "print(df_promotion_lift['promotion_lift_pct'].describe())\n",
        "print(f\"\\nPromotion type distribution:\")\n",
        "print(df_promotion_lift['promotion_type'].value_counts())\n",
        "\n",
        "display(df_promotion_lift.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save to Delta table\n",
        "spark.createDataFrame(df_promotion_lift).write.mode(\"overwrite\").saveAsTable(\"promotion_lift\")\n",
        "print(f\"✓ Saved to {CATALOG}.{SCHEMA}.promotion_lift\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Scrap Anomaly Dataset (Anomaly Detection)\n",
        "\n",
        "**Use Case:** Production Planning - Detect unusual scrap/defect patterns\n",
        "\n",
        "This dataset helps production managers identify abnormal production runs that may indicate equipment issues, material problems, or process deviations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate scrap anomaly data\n",
        "df_scrap_anomaly, anomaly_labels = generate_scrap_anomaly_data(n_samples=1000, anomaly_rate=0.08, seed=42)\n",
        "\n",
        "# Add the labels to the dataframe\n",
        "df_scrap_anomaly['is_anomaly'] = anomaly_labels\n",
        "\n",
        "print(f\"Scrap Anomaly Dataset\")\n",
        "print(f\"Shape: {df_scrap_anomaly.shape}\")\n",
        "print(f\"\\nAnomaly distribution:\")\n",
        "print(f\"Normal (0): {(anomaly_labels == 0).sum()}\")\n",
        "print(f\"Anomaly (1): {(anomaly_labels == 1).sum()}\")\n",
        "print(f\"Anomaly rate: {anomaly_labels.mean():.1%}\")\n",
        "\n",
        "display(df_scrap_anomaly.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save to Delta table\n",
        "spark.createDataFrame(df_scrap_anomaly).write.mode(\"overwrite\").saveAsTable(\"scrap_anomaly\")\n",
        "print(f\"✓ Saved to {CATALOG}.{SCHEMA}.scrap_anomaly\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Demand Forecast Dataset (Time Series)\n",
        "\n",
        "**Use Case:** Demand Planning - Forecast product demand by category and region\n",
        "\n",
        "This dataset contains monthly demand data across multiple product categories and regions, supporting demand forecasting and inventory planning processes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate demand forecast data\n",
        "df_demand_forecast = generate_aggregate_demand_forecast_data(n_series=50, n_months=36, seed=42)\n",
        "\n",
        "print(f\"Demand Forecast Dataset\")\n",
        "print(f\"Shape: {df_demand_forecast.shape}\")\n",
        "print(f\"Number of time series: {df_demand_forecast['series_id'].nunique()}\")\n",
        "print(f\"Time range: {df_demand_forecast['date'].min()} to {df_demand_forecast['date'].max()}\")\n",
        "print(f\"\\nCategory distribution:\")\n",
        "print(df_demand_forecast['category'].value_counts())\n",
        "print(f\"\\nRegion distribution:\")\n",
        "print(df_demand_forecast['region'].value_counts())\n",
        "\n",
        "display(df_demand_forecast.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save to Delta table\n",
        "spark.createDataFrame(df_demand_forecast).write.mode(\"overwrite\").saveAsTable(\"demand_forecast\")\n",
        "print(f\"✓ Saved to {CATALOG}.{SCHEMA}.demand_forecast\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Verify All Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all tables in the schema\n",
        "print(f\"Tables in {CATALOG}.{SCHEMA}:\")\n",
        "display(spark.sql(f\"SHOW TABLES IN {CATALOG}.{SCHEMA}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview each table\n",
        "tables = [\n",
        "    (\"supplier_delay_risk\", \"Supply Planning - Predict delivery delays\"),\n",
        "    (\"material_shortage\", \"Material Planning - Predict shortage risk\"),\n",
        "    (\"price_elasticity\", \"Demand Planning - Price sensitivity\"),\n",
        "    (\"promotion_lift\", \"Demand Planning - Promotion impact\"),\n",
        "    (\"scrap_anomaly\", \"Production Planning - Detect anomalies\"),\n",
        "    (\"demand_forecast\", \"Demand Planning - Time series forecasting\")\n",
        "]\n",
        "\n",
        "for table_name, description in tables:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{table_name}: {description}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    display(spark.table(table_name).limit(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "All datasets have been prepared and saved as Delta tables in `tabpfn_databricks.default`:\n",
        "\n",
        "| Table | Task | Planning Process | Samples | Features |\n",
        "|-------|------|------------------|---------|----------|\n",
        "| `supplier_delay_risk` | Binary Classification | Supply Planning | 2,000 | 14 |\n",
        "| `material_shortage` | Multi-class Classification | Material Planning | 1,500 | 15 |\n",
        "| `price_elasticity` | Regression | Demand Planning | 3,000 | 13 |\n",
        "| `promotion_lift` | Regression | Demand Planning | 2,500 | 15 |\n",
        "| `scrap_anomaly` | Anomaly Detection | Production Planning | 1,000 | 13 |\n",
        "| `demand_forecast` | Time Series Forecasting | Demand Planning | 1,800 | 7 |\n",
        "\n",
        "**Next steps:** Run the individual notebooks (01-04) to explore TabPFN capabilities using these prepared datasets.\n",
        "\n",
        "### Planning Value Chain Coverage\n",
        "\n",
        "```\n",
        "┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n",
        "│ Demand Planning │───▶│ Supply Planning │───▶│ Production      │\n",
        "│                 │    │                 │    │ Planning        │\n",
        "│ • Forecasting   │    │ • Supplier Risk │    │ • Yield Pred.   │\n",
        "│ • Price Elast.  │    │ • Lead Time     │    │ • Scrap Detect. │\n",
        "│ • Promo Lift    │    │ • Material      │    │                 │\n",
        "│                 │    │   Shortage      │    │                 │\n",
        "└─────────────────┘    └─────────────────┘    └─────────────────┘\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
