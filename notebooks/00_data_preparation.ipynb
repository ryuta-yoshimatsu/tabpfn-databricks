{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preparation for Retail/CPG Planning Analytics\n",
        "\n",
        "This notebook generates and prepares realistic datasets for a **global retail/CPG company's planning value chain**. The datasets cover end-to-end planning processes from demand planning to supply planning.\n",
        "\n",
        "**Planning Value Chain Covered:**\n",
        "- Demand Planning\n",
        "- Supply Planning\n",
        "- Production Planning\n",
        "- Distribution Planning\n",
        "\n",
        "**Datasets Generated:**\n",
        "\n",
        "| Dataset | Type | Planning Process | Notebook |\n",
        "|---------|------|------------------|----------|\n",
        "| `demand_forecast` | Time Series | Demand Planning | 04_time_series_forecasting |\n",
        "| `price_elasticity` | Regression | Demand Planning | 02_regression |\n",
        "| `promotion_lift` | Regression | Demand Planning | 02_regression |\n",
        "| `supplier_delay_risk` | Binary Classification | Supply Planning | 01_classification |\n",
        "| `supplier_lead_time` | Regression | Supply Planning | 02_regression |\n",
        "| `material_shortage` | Multi-class Classification | Supply Planning | 01_classification |\n",
        "| `labor_shortage` | Multi-class Classification | Production Planning | 01_classification |\n",
        "| `yield_prediction` | Regression | Production Planning | 02_regression |\n",
        "| `scrap_anomaly` | Anomaly Detection | Production Planning | 03_outlier_detection |\n",
        "| `transportation_lead_time` | Regression | Distribution Planning | 02_regression |\n",
        "| `otif_risk` | Multi-class Classification | Distribution Planning | 01_classification |\n",
        "\n",
        "**Run this notebook once** to set up all the data before running the analytics notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute Setup\n",
        "\n",
        "We recommend running this notebook on **Serverless Compute** with the **Base Environment V4**.\n",
        "\n",
        "To configure:\n",
        "1. Click on the compute selector in the notebook toolbar\n",
        "2. Select **Serverless**\n",
        "3. Under Environment, choose **Base Environment V4**\n",
        "\n",
        "Serverless compute provides fast startup times and automatic scaling, ideal for interactive notebook workflows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install pandas numpy --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dbutils.library.restartPython()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "\n",
        "Define the catalog and schema where datasets will be stored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure your catalog and schema\n",
        "CATALOG = \"tabpfn_databricks\"\n",
        "SCHEMA = \"default\"\n",
        "\n",
        "# Create the catalog and schema if they don't exist\n",
        "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
        "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
        "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
        "spark.sql(f\"USE SCHEMA {SCHEMA}\")\n",
        "\n",
        "print(f\"Using catalog: {CATALOG}\")\n",
        "print(f\"Using schema: {SCHEMA}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Import Data Generation Utilities\n",
        "\n",
        "We use custom data generation functions that create realistic retail/CPG planning datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the scripts directory to the path so we can import util.py\n",
        "# In Databricks, you may need to adjust this path based on your repo structure\n",
        "repo_root = os.path.dirname(os.getcwd()) if 'notebooks' in os.getcwd() else os.getcwd()\n",
        "scripts_path = os.path.join(repo_root, 'scripts')\n",
        "if scripts_path not in sys.path:\n",
        "    sys.path.insert(0, scripts_path)\n",
        "\n",
        "# Import data generation functions\n",
        "from util import (\n",
        "    generate_supplier_delay_risk_data,\n",
        "    generate_material_shortage_data,\n",
        "    generate_labor_shortage_data,\n",
        "    generate_price_elasticity_data,\n",
        "    generate_promotion_lift_data,\n",
        "    generate_supplier_lead_time_data,\n",
        "    generate_transportation_lead_time_data,\n",
        "    generate_yield_prediction_data,\n",
        "    generate_scrap_anomaly_data,\n",
        "    generate_aggregate_demand_forecast_data,\n",
        "    generate_otif_risk_data\n",
        ")\n",
        "\n",
        "print(\"Data generation utilities imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Demand Planning Datasets\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Demand Forecast Dataset (Time Series)\n",
        "\n",
        "**Use Case:** Forecast product demand by category and region\n",
        "\n",
        "This dataset contains monthly demand data across multiple product categories and regions, supporting demand forecasting and inventory planning processes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate demand forecast data\n",
        "df_demand_forecast = generate_aggregate_demand_forecast_data(n_series=50, n_months=36, seed=42)\n",
        "\n",
        "print(f\"Demand Forecast Dataset\")\n",
        "print(f\"Shape: {df_demand_forecast.shape}\")\n",
        "print(f\"Number of time series: {df_demand_forecast['series_id'].nunique()}\")\n",
        "print(f\"Time range: {df_demand_forecast['date'].min()} to {df_demand_forecast['date'].max()}\")\n",
        "print(f\"\\nCategory distribution:\")\n",
        "print(df_demand_forecast['category'].value_counts())\n",
        "\n",
        "display(df_demand_forecast.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.createDataFrame(df_demand_forecast).write.mode(\"overwrite\").saveAsTable(\"demand_forecast\")\n",
        "print(f\"✓ Saved to {CATALOG}.{SCHEMA}.demand_forecast\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Price Elasticity Dataset (Regression)\n",
        "\n",
        "**Use Case:** Understand how price changes affect demand\n",
        "\n",
        "This dataset helps demand planners and revenue management teams predict how changes in pricing will impact unit sales across different products and market conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_price_elasticity = generate_price_elasticity_data(n_samples=3000, seed=42)\n",
        "\n",
        "print(f\"Price Elasticity Dataset\")\n",
        "print(f\"Shape: {df_price_elasticity.shape}\")\n",
        "print(f\"\\nTarget (price_elasticity) statistics:\")\n",
        "print(df_price_elasticity['price_elasticity'].describe())\n",
        "print(f\"\\nNote: Elasticity values are typically negative (demand decreases as price increases)\")\n",
        "\n",
        "display(df_price_elasticity.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.createDataFrame(df_price_elasticity).write.mode(\"overwrite\").saveAsTable(\"price_elasticity\")\n",
        "print(f\"✓ Saved to {CATALOG}.{SCHEMA}.price_elasticity\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Promotion Lift Dataset (Regression)\n",
        "\n",
        "**Use Case:** Predict the sales impact of planned promotions\n",
        "\n",
        "This dataset supports trade promotion planning by predicting the incremental sales lift from different promotion types, depths, and marketing support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_promotion_lift = generate_promotion_lift_data(n_samples=2500, seed=42)\n",
        "\n",
        "print(f\"Promotion Lift Dataset\")\n",
        "print(f\"Shape: {df_promotion_lift.shape}\")\n",
        "print(f\"\\nTarget (promotion_lift_pct) statistics:\")\n",
        "print(df_promotion_lift['promotion_lift_pct'].describe())\n",
        "print(f\"\\nPromotion type distribution:\")\n",
        "print(df_promotion_lift['promotion_type'].value_counts())\n",
        "\n",
        "display(df_promotion_lift.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.createDataFrame(df_promotion_lift).write.mode(\"overwrite\").saveAsTable(\"promotion_lift\")\n",
        "print(f\"✓ Saved to {CATALOG}.{SCHEMA}.promotion_lift\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Supply Planning Datasets\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Supplier Delay Risk Dataset (Binary Classification)\n",
        "\n",
        "**Use Case:** Predict which supplier deliveries are at risk of delay\n",
        "\n",
        "This dataset helps supply planners identify high-risk orders and take proactive mitigation actions such as expediting, finding alternative suppliers, or adjusting production schedules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_supplier_delay = generate_supplier_delay_risk_data(n_samples=2000, seed=42)\n",
        "\n",
        "print(f\"Supplier Delay Risk Dataset\")\n",
        "print(f\"Shape: {df_supplier_delay.shape}\")\n",
        "print(f\"\\nTarget distribution (is_delayed):\")\n",
        "print(df_supplier_delay['is_delayed'].value_counts())\n",
        "print(f\"\\nDelay rate: {df_supplier_delay['is_delayed'].mean():.1%}\")\n",
        "\n",
        "display(df_supplier_delay.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.createDataFrame(df_supplier_delay).write.mode(\"overwrite\").saveAsTable(\"supplier_delay_risk\")\n",
        "print(f\"✓ Saved to {CATALOG}.{SCHEMA}.supplier_delay_risk\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Supplier Lead Time Dataset (Regression)\n",
        "\n",
        "**Use Case:** Predict actual supplier delivery lead times\n",
        "\n",
        "This dataset helps supply planners predict actual delivery times (vs. contracted lead times) to improve planning accuracy and reduce stockouts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_supplier_lead_time = generate_supplier_lead_time_data(n_samples=2000, seed=42)\n",
        "\n",
        "print(f\"Supplier Lead Time Dataset\")\n",
        "print(f\"Shape: {df_supplier_lead_time.shape}\")\n",
        "print(f\"\\nTarget (actual_lead_time_days) statistics:\")\n",
        "print(df_supplier_lead_time['actual_lead_time_days'].describe())\n",
        "print(f\"\\nSupplier region distribution:\")\n",
        "print(df_supplier_lead_time['supplier_region'].value_counts())\n",
        "\n",
        "display(df_supplier_lead_time.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.createDataFrame(df_supplier_lead_time).write.mode(\"overwrite\").saveAsTable(\"supplier_lead_time\")\n",
        "print(f\"✓ Saved to {CATALOG}.{SCHEMA}.supplier_lead_time\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Material Shortage Dataset (Multi-class Classification)\n",
        "\n",
        "**Use Case:** Predict which materials are at risk of shortage\n",
        "\n",
        "This dataset supports material planners in prioritizing procurement actions based on shortage risk levels (No Risk, At Risk, Critical)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_material_shortage = generate_material_shortage_data(n_samples=1500, seed=42)\n",
        "\n",
        "print(f\"Material Shortage Dataset\")\n",
        "print(f\"Shape: {df_material_shortage.shape}\")\n",
        "print(f\"\\nTarget distribution (shortage_risk):\")\n",
        "print(\"0 = No Risk, 1 = At Risk, 2 = Critical\")\n",
        "print(df_material_shortage['shortage_risk'].value_counts().sort_index())\n",
        "\n",
        "display(df_material_shortage.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.createDataFrame(df_material_shortage).write.mode(\"overwrite\").saveAsTable(\"material_shortage\")\n",
        "print(f\"✓ Saved to {CATALOG}.{SCHEMA}.material_shortage\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Production Planning Datasets\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Labor Shortage Dataset (Multi-class Classification)\n",
        "\n",
        "**Use Case:** Predict labor availability issues at facilities\n",
        "\n",
        "This dataset helps production and HR planners anticipate workforce shortages and take proactive actions like overtime scheduling, temp staffing, or cross-training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_labor_shortage = generate_labor_shortage_data(n_samples=1500, seed=42)\n",
        "\n",
        "print(f\"Labor Shortage Dataset\")\n",
        "print(f\"Shape: {df_labor_shortage.shape}\")\n",
        "print(f\"\\nTarget distribution (labor_shortage_risk):\")\n",
        "print(\"0 = Adequate, 1 = At Risk, 2 = Critical\")\n",
        "print(df_labor_shortage['labor_shortage_risk'].value_counts().sort_index())\n",
        "print(f\"\\nFacility type distribution:\")\n",
        "print(df_labor_shortage['facility_type'].value_counts())\n",
        "\n",
        "display(df_labor_shortage.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.createDataFrame(df_labor_shortage).write.mode(\"overwrite\").saveAsTable(\"labor_shortage\")\n",
        "print(f\"✓ Saved to {CATALOG}.{SCHEMA}.labor_shortage\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Yield Prediction Dataset (Regression)\n",
        "\n",
        "**Use Case:** Predict production yield for capacity planning\n",
        "\n",
        "This dataset helps production planners predict output yield based on equipment, materials, and process parameters to optimize capacity planning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_yield = generate_yield_prediction_data(n_samples=2000, seed=42)\n",
        "\n",
        "print(f\"Yield Prediction Dataset\")\n",
        "print(f\"Shape: {df_yield.shape}\")\n",
        "print(f\"\\nTarget (yield_percentage) statistics:\")\n",
        "print(df_yield['yield_percentage'].describe())\n",
        "print(f\"\\nProduction line distribution:\")\n",
        "print(df_yield['production_line'].value_counts())\n",
        "\n",
        "display(df_yield.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.createDataFrame(df_yield).write.mode(\"overwrite\").saveAsTable(\"yield_prediction\")\n",
        "print(f\"✓ Saved to {CATALOG}.{SCHEMA}.yield_prediction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Scrap Anomaly Dataset (Anomaly Detection)\n",
        "\n",
        "**Use Case:** Detect unusual scrap/defect patterns\n",
        "\n",
        "This dataset helps production managers identify abnormal production runs that may indicate equipment issues, material problems, or process deviations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_scrap_anomaly, anomaly_labels = generate_scrap_anomaly_data(n_samples=1000, anomaly_rate=0.08, seed=42)\n",
        "df_scrap_anomaly['is_anomaly'] = anomaly_labels\n",
        "\n",
        "print(f\"Scrap Anomaly Dataset\")\n",
        "print(f\"Shape: {df_scrap_anomaly.shape}\")\n",
        "print(f\"\\nAnomaly distribution:\")\n",
        "print(f\"Normal (0): {(anomaly_labels == 0).sum()}\")\n",
        "print(f\"Anomaly (1): {(anomaly_labels == 1).sum()}\")\n",
        "print(f\"Anomaly rate: {anomaly_labels.mean():.1%}\")\n",
        "\n",
        "display(df_scrap_anomaly.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.createDataFrame(df_scrap_anomaly).write.mode(\"overwrite\").saveAsTable(\"scrap_anomaly\")\n",
        "print(f\"✓ Saved to {CATALOG}.{SCHEMA}.scrap_anomaly\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Distribution Planning Datasets\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Transportation Lead Time Dataset (Regression)\n",
        "\n",
        "**Use Case:** Predict transportation/delivery lead times\n",
        "\n",
        "This dataset helps distribution planners predict actual transit times for shipments to improve delivery planning and customer service levels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_transport_lead_time = generate_transportation_lead_time_data(n_samples=2000, seed=42)\n",
        "\n",
        "print(f\"Transportation Lead Time Dataset\")\n",
        "print(f\"Shape: {df_transport_lead_time.shape}\")\n",
        "print(f\"\\nTarget (actual_transit_days) statistics:\")\n",
        "print(df_transport_lead_time['actual_transit_days'].describe())\n",
        "print(f\"\\nShipment type distribution:\")\n",
        "print(df_transport_lead_time['shipment_type'].value_counts())\n",
        "\n",
        "display(df_transport_lead_time.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.createDataFrame(df_transport_lead_time).write.mode(\"overwrite\").saveAsTable(\"transportation_lead_time\")\n",
        "print(f\"✓ Saved to {CATALOG}.{SCHEMA}.transportation_lead_time\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. OTIF Risk Dataset (Multi-class Classification)\n",
        "\n",
        "**Use Case:** Predict On-Time-In-Full (OTIF) delivery risk\n",
        "\n",
        "This dataset helps distribution planners identify orders at risk of failing to be delivered on-time and in-full, enabling proactive intervention to improve customer service levels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_otif_risk = generate_otif_risk_data(n_samples=2000, seed=42)\n",
        "\n",
        "print(f\"OTIF Risk Dataset\")\n",
        "print(f\"Shape: {df_otif_risk.shape}\")\n",
        "print(f\"\\nTarget distribution (otif_risk):\")\n",
        "print(\"0 = Low Risk, 1 = Medium Risk, 2 = High Risk\")\n",
        "print(df_otif_risk['otif_risk'].value_counts().sort_index())\n",
        "print(f\"\\nOrder type distribution:\")\n",
        "print(df_otif_risk['order_type'].value_counts())\n",
        "\n",
        "display(df_otif_risk.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.createDataFrame(df_otif_risk).write.mode(\"overwrite\").saveAsTable(\"otif_risk\")\n",
        "print(f\"✓ Saved to {CATALOG}.{SCHEMA}.otif_risk\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 15. Verify All Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all tables in the schema\n",
        "print(f\"Tables in {CATALOG}.{SCHEMA}:\")\n",
        "display(spark.sql(f\"SHOW TABLES IN {CATALOG}.{SCHEMA}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview each table\n",
        "tables = [\n",
        "    (\"demand_forecast\", \"Demand Planning - Time series forecasting\"),\n",
        "    (\"price_elasticity\", \"Demand Planning - Price sensitivity\"),\n",
        "    (\"promotion_lift\", \"Demand Planning - Promotion impact\"),\n",
        "    (\"supplier_delay_risk\", \"Supply Planning - Predict delivery delays\"),\n",
        "    (\"supplier_lead_time\", \"Supply Planning - Predict lead times\"),\n",
        "    (\"material_shortage\", \"Supply Planning - Predict shortage risk\"),\n",
        "    (\"labor_shortage\", \"Production Planning - Predict labor shortages\"),\n",
        "    (\"yield_prediction\", \"Production Planning - Predict yield\"),\n",
        "    (\"scrap_anomaly\", \"Production Planning - Detect anomalies\"),\n",
        "    (\"transportation_lead_time\", \"Distribution Planning - Predict transit times\"),\n",
        "    (\"otif_risk\", \"Distribution Planning - Predict OTIF risk\"),\n",
        "]\n",
        "\n",
        "for table_name, description in tables:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{table_name}: {description}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    display(spark.table(table_name).limit(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "All datasets have been prepared and saved as Delta tables in `tabpfn_databricks.default`:\n",
        "\n",
        "| Table | Task | Planning Process | Samples |\n",
        "|-------|------|------------------|--------|\n",
        "| `demand_forecast` | Time Series | Demand Planning | 1,800 |\n",
        "| `price_elasticity` | Regression | Demand Planning | 3,000 |\n",
        "| `promotion_lift` | Regression | Demand Planning | 2,500 |\n",
        "| `supplier_delay_risk` | Binary Classification | Supply Planning | 2,000 |\n",
        "| `supplier_lead_time` | Regression | Supply Planning | 2,000 |\n",
        "| `material_shortage` | Multi-class Classification | Supply Planning | 1,500 |\n",
        "| `labor_shortage` | Multi-class Classification | Production Planning | 1,500 |\n",
        "| `yield_prediction` | Regression | Production Planning | 2,000 |\n",
        "| `scrap_anomaly` | Anomaly Detection | Production Planning | 1,000 |\n",
        "| `transportation_lead_time` | Regression | Distribution Planning | 2,000 |\n",
        "| `otif_risk` | Multi-class Classification | Distribution Planning | 2,000 |\n",
        "\n",
        "**Next steps:** Run the individual notebooks (01-04) to explore TabPFN capabilities using these prepared datasets.\n",
        "\n",
        "### Planning Value Chain Coverage\n",
        "\n",
        "```\n",
        "┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n",
        "│ Demand Planning │───▶│ Supply Planning │───▶│ Production      │───▶│ Distribution    │\n",
        "│                 │    │                 │    │ Planning        │    │ Planning        │\n",
        "│ • Forecasting   │    │ • Supplier Risk │    │ • Labor Short.  │    │ • Transport LT  │\n",
        "│ • Price Elast.  │    │ • Supplier LT   │    │ • Yield Pred.   │    │ • OTIF Risk     │\n",
        "│ • Promo Lift    │    │ • Material      │    │ • Scrap Detect. │    │                 │\n",
        "│                 │    │   Shortage      │    │                 │    │                 │\n",
        "└─────────────────┘    └─────────────────┘    └─────────────────┘    └─────────────────┘\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
