{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Production Anomaly Detection with TabPFN\n",
        "\n",
        "This notebook demonstrates outlier detection techniques for production planning in retail/CPG manufacturing.\n",
        "\n",
        "**Use Case:** Production Planning - Detect unusual scrap/defect patterns\n",
        "\n",
        "**Business Context:** Manufacturing operations generate vast amounts of production data. Detecting anomalous production runs early helps:\n",
        "- Identify equipment issues before major failures\n",
        "- Catch quality problems early in the production process\n",
        "- Reduce scrap costs and improve overall equipment effectiveness (OEE)\n",
        "- Maintain product quality and customer satisfaction\n",
        "\n",
        "**What you will learn:**\n",
        "- How to use TabPFN for anomaly scoring via semi-supervised classification\n",
        "- How to evaluate and visualize anomaly scores\n",
        "- How to identify anomalous production runs and understand key indicators\n",
        "\n",
        "**Prerequisites:** Run `00_data_preparation` notebook first to set up the datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute Setup\n",
        "\n",
        "We recommend running this notebook on **Serverless Compute** with the **Base Environment V4**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install tabpfn-client scikit-learn pandas matplotlib seaborn mlflow --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dbutils.library.restartPython()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Authentication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tabpfn_client\n",
        "\n",
        "token = dbutils.secrets.get(scope=\"tabpfn-client\", key=\"token\")\n",
        "tabpfn_client.set_access_token(token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CATALOG = \"tabpfn_databricks\"\n",
        "SCHEMA = \"default\"\n",
        "\n",
        "# MLflow experiment configuration (shared across all TabPFN notebooks)\n",
        "# Default uses user namespace, but can be customized\n",
        "current_user = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
        "MLFLOW_EXPERIMENT_NAME = f\"/Users/{current_user}/tabpfn-databricks\"\n",
        "\n",
        "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
        "spark.sql(f\"USE SCHEMA {SCHEMA}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, confusion_matrix\n",
        "import mlflow\n",
        "\n",
        "from tabpfn_client import TabPFNClassifier\n",
        "\n",
        "# Set MLflow experiment\n",
        "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
        "print(f\"MLflow experiment set to: {MLFLOW_EXPERIMENT_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load Scrap Anomaly Data\n",
        "\n",
        "The scrap anomaly dataset contains production metrics that can indicate abnormal production runs:\n",
        "- **Scrap rate**: Percentage of units scrapped\n",
        "- **Defect count**: Number of defects detected\n",
        "- **Rework hours**: Time spent on rework\n",
        "- **Equipment vibration**: Sensor reading for equipment health\n",
        "- **Process temperature deviation**: Deviation from optimal temperature\n",
        "- **Quality score**: Overall quality assessment\n",
        "\n",
        "Anomalies may indicate:\n",
        "- Equipment malfunction\n",
        "- Material quality issues\n",
        "- Process parameter drift\n",
        "- Operator errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Scrap Anomaly training dataset from Delta table\n",
        "df_scrap = spark.table(\"scrap_anomaly_train\").toPandas()\n",
        "\n",
        "# Extract labels and features\n",
        "y_true = df_scrap['is_anomaly'].values\n",
        "\n",
        "# Select numeric features for anomaly detection\n",
        "numeric_features = [\n",
        "    'scrap_rate_pct', 'defect_count', 'rework_hours', 'equipment_vibration',\n",
        "    'process_temperature_deviation', 'material_waste_pct', 'cycle_time_variance',\n",
        "    'operator_interventions', 'quality_score', 'downtime_minutes'\n",
        "]\n",
        "\n",
        "print(f\"Dataset shape: {df_scrap.shape}\")\n",
        "print(f\"\\nAnomaly distribution:\")\n",
        "print(f\"  Normal (0): {(y_true == 0).sum()}\")\n",
        "print(f\"  Anomaly (1): {(y_true == 1).sum()}\")\n",
        "print(f\"  Anomaly rate: {y_true.mean():.1%}\")\n",
        "\n",
        "display(df_scrap.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize distributions for key features\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "features_to_plot = ['scrap_rate_pct', 'defect_count', 'equipment_vibration', \n",
        "                    'quality_score', 'rework_hours', 'downtime_minutes']\n",
        "\n",
        "for ax, feature in zip(axes.flatten(), features_to_plot):\n",
        "    # Plot distribution by anomaly status\n",
        "    df_scrap[df_scrap['is_anomaly'] == 0][feature].hist(ax=ax, bins=30, alpha=0.5, \n",
        "                                                        label='Normal', color='blue')\n",
        "    df_scrap[df_scrap['is_anomaly'] == 1][feature].hist(ax=ax, bins=30, alpha=0.5, \n",
        "                                                        label='Anomaly', color='red')\n",
        "    ax.set_title(feature)\n",
        "    ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. TabPFN-based Anomaly Detection\n",
        "\n",
        "We use TabPFN as a classifier in a semi-supervised setup:\n",
        "1. Train on a subset of labeled normal data plus some labeled anomalies\n",
        "2. Use the model to score all production runs\n",
        "3. Higher anomaly probability = more likely to be abnormal\n",
        "\n",
        "This approach leverages TabPFN's strong classification capabilities for anomaly detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare feature matrix\n",
        "X = df_scrap[numeric_features].values\n",
        "\n",
        "# Create a semi-supervised training set\n",
        "# Use 70% of the data for \"training\" (fitting the anomaly detector)\n",
        "np.random.seed(42)\n",
        "n_total = len(X)\n",
        "n_train = int(0.7 * n_total)\n",
        "\n",
        "# Shuffle indices\n",
        "shuffled_idx = np.random.permutation(n_total)\n",
        "train_idx = shuffled_idx[:n_train]\n",
        "test_idx = shuffled_idx[n_train:]\n",
        "\n",
        "X_train = X[train_idx]\n",
        "y_train = y_true[train_idx]\n",
        "X_test = X[test_idx]\n",
        "y_test = y_true[test_idx]\n",
        "\n",
        "print(f\"Training set: {len(X_train)} samples ({y_train.mean():.1%} anomalies)\")\n",
        "print(f\"Test set: {len(X_test)} samples ({y_test.mean():.1%} anomalies)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train TabPFN classifier for anomaly detection with MLflow logging\n",
        "with mlflow.start_run(run_name=\"scrap_anomaly_tabpfn\"):\n",
        "    # Log parameters\n",
        "    mlflow.log_param(\"model_type\", \"TabPFNClassifier\")\n",
        "    mlflow.log_param(\"task\", \"scrap_anomaly_detection\")\n",
        "    mlflow.log_param(\"approach\", \"semi_supervised\")\n",
        "    mlflow.log_param(\"train_ratio\", 0.7)\n",
        "    mlflow.log_param(\"n_features\", X_train.shape[1])\n",
        "    mlflow.log_param(\"train_samples\", X_train.shape[0])\n",
        "    mlflow.log_param(\"test_samples\", X_test.shape[0])\n",
        "    mlflow.log_param(\"train_anomaly_rate\", y_train.mean())\n",
        "    mlflow.log_param(\"test_anomaly_rate\", y_test.mean())\n",
        "    \n",
        "    clf = TabPFNClassifier()\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Score test set - probability of being an anomaly\n",
        "    anomaly_scores_tabpfn = clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Evaluate\n",
        "    roc_auc_tabpfn = roc_auc_score(y_test, anomaly_scores_tabpfn)\n",
        "    \n",
        "    # Calculate PR AUC\n",
        "    precision, recall, _ = precision_recall_curve(y_test, anomaly_scores_tabpfn)\n",
        "    pr_auc_tabpfn = auc(recall, precision)\n",
        "    \n",
        "    # Log metrics\n",
        "    mlflow.log_metric(\"roc_auc\", roc_auc_tabpfn)\n",
        "    mlflow.log_metric(\"pr_auc\", pr_auc_tabpfn)\n",
        "    \n",
        "    print(f\"TabPFN ROC AUC: {roc_auc_tabpfn:.4f}\")\n",
        "    print(f\"TabPFN PR AUC: {pr_auc_tabpfn:.4f}\")\n",
        "    print(f\"MLflow Run ID: {mlflow.active_run().info.run_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize anomaly score distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Score distribution by actual label\n",
        "axes[0].hist(anomaly_scores_tabpfn[y_test == 0], bins=30, alpha=0.5, \n",
        "             label='Normal', color='blue', density=True)\n",
        "axes[0].hist(anomaly_scores_tabpfn[y_test == 1], bins=30, alpha=0.5, \n",
        "             label='Anomaly', color='red', density=True)\n",
        "axes[0].set_xlabel('Anomaly Score (Probability)')\n",
        "axes[0].set_ylabel('Density')\n",
        "axes[0].set_title('TabPFN Anomaly Score Distribution')\n",
        "axes[0].legend()\n",
        "\n",
        "# Precision-Recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, anomaly_scores_tabpfn)\n",
        "pr_auc = auc(recall, precision)\n",
        "axes[1].plot(recall, precision, 'b-', linewidth=2)\n",
        "axes[1].set_xlabel('Recall')\n",
        "axes[1].set_ylabel('Precision')\n",
        "axes[1].set_title(f'Precision-Recall Curve (AUC = {pr_auc:.4f})')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Identifying Anomalous Production Runs\n",
        "\n",
        "Let's identify the most anomalous production runs and understand what makes them unusual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add anomaly scores to test data\n",
        "df_test = df_scrap.iloc[test_idx].copy().reset_index(drop=True)\n",
        "df_test['anomaly_score'] = anomaly_scores_tabpfn\n",
        "df_test['predicted_anomaly'] = (anomaly_scores_tabpfn > 0.5).astype(int)\n",
        "\n",
        "# Show top 10 most anomalous production runs\n",
        "print(\"Top 10 Most Anomalous Production Runs:\")\n",
        "top_anomalies = df_test.nlargest(10, 'anomaly_score')[[\n",
        "    'batch_number', 'production_line', 'shift', 'scrap_rate_pct', \n",
        "    'defect_count', 'equipment_vibration', 'quality_score', \n",
        "    'anomaly_score', 'is_anomaly'\n",
        "]]\n",
        "display(top_anomalies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrix at 0.5 threshold\n",
        "y_pred = (anomaly_scores_tabpfn > 0.5).astype(int)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "im = ax.imshow(cm, interpolation='nearest', cmap='Blues')\n",
        "ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "classes = ['Normal', 'Anomaly']\n",
        "ax.set(xticks=[0, 1], yticks=[0, 1],\n",
        "       xticklabels=classes, yticklabels=classes,\n",
        "       title='Anomaly Detection Confusion Matrix',\n",
        "       ylabel='Actual', xlabel='Predicted')\n",
        "\n",
        "# Add text annotations\n",
        "thresh = cm.max() / 2.\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        ax.text(j, i, format(cm[i, j], 'd'),\n",
        "                ha=\"center\", va=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate detection metrics\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "precision_val = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "recall_val = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "f1_val = 2 * precision_val * recall_val / (precision_val + recall_val) if (precision_val + recall_val) > 0 else 0\n",
        "\n",
        "print(f\"\\nDetection Metrics (threshold=0.5):\")\n",
        "print(f\"  Precision: {precision_val:.3f}\")\n",
        "print(f\"  Recall: {recall_val:.3f}\")\n",
        "print(f\"  F1 Score: {f1_val:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance: Which features contribute most to anomaly detection?\n",
        "# Calculate mean difference between anomalies and normal samples\n",
        "df_normal = df_scrap[df_scrap['is_anomaly'] == 0][numeric_features]\n",
        "df_anomaly = df_scrap[df_scrap['is_anomaly'] == 1][numeric_features]\n",
        "\n",
        "# Standardized difference\n",
        "mean_diff = (df_anomaly.mean() - df_normal.mean()) / df_normal.std()\n",
        "mean_diff = mean_diff.sort_values(key=abs, ascending=True)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "colors = ['#e74c3c' if v > 0 else '#3498db' for v in mean_diff.values]\n",
        "mean_diff.plot(kind='barh', ax=ax, color=colors)\n",
        "ax.set_xlabel('Standardized Difference (Anomaly - Normal)')\n",
        "ax.set_title('Feature Differences: Anomalies vs Normal Production Runs')\n",
        "ax.axvline(x=0, color='black', linestyle='--', linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKey Anomaly Indicators (highest absolute difference):\")\n",
        "top_indicators = mean_diff.abs().nlargest(5)\n",
        "for feature in top_indicators.index:\n",
        "    direction = \"higher\" if mean_diff[feature] > 0 else \"lower\"\n",
        "    print(f\"  - {feature}: Anomalies have {direction} values\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, we demonstrated:\n",
        "\n",
        "- **TabPFN for Anomaly Detection** - Semi-supervised approach using classification\n",
        "- **Production Anomaly Identification** - Finding unusual production runs\n",
        "- **Feature Analysis** - Understanding what makes runs anomalous\n",
        "\n",
        "**Key Takeaways:**\n",
        "1. TabPFN's semi-supervised approach leverages labeled anomaly data effectively\n",
        "2. Probability scores enable threshold tuning for different business objectives\n",
        "\n",
        "**Next Steps:**\n",
        "- Run `04_time_series_forecasting` notebook for demand forecasting\n",
        "- Integrate real-time anomaly scoring into production monitoring systems\n",
        "- Develop automated alerting workflows for high-risk production runs"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
